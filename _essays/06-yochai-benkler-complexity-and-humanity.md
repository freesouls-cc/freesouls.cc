---
title: Complexity and Humanity
author: Yochai Benkler
portrait: http://flickr.com/photos/joi/536642630/
image: https://farm2.static.flickr.com/1250/536642630_dca30f8d4d.jpg
bio: >
  Yochai Benkler is Jack N. and Lillian R. Berkman Professor for
  Entrepreneurial Legal Studies at Harvard Law School, and the author of
  The Wealth of Networks and the paper "Coase's Penguin."
source: From Wikipedia, the free encyclopedia
links:
    - name: website
      title: Yochai Benkler
      url: http://benkler.org
chinese: /yeeyan/complexity-humanity-by-yochai-benkler.html
---

> "There is always a little bit of heaven in a disaster area."<br/>
>  −Wavy Gravy

We have all seen the images. Volunteers pitching in. People working day
and night; coming up with the most ingenious, improvised solutions to
everything from food and shelter to communications and security. Working
together; patching up the fabric that is rent. Disaster, natural or
otherwise, is a breakdown of systems. For a time, chaos reigns. For a
time, what will happen in the next five minutes, five hours, and five
days is unknown. All we have to rely on are our wits, fortitude, and
common humanity

Contemporary life is not chaotic, in the colloquial sense we apply to
disaster zones. It is, however, complex and rapidly changing; much more
so than life was in the past; even the very near past. Life, of course,
was never simple. But the fact that day-to-day behaviors in Shenzhen and
Bangalore have direct and immediate effects on people from Wichita to
Strasbourg, from Rio de Janeiro to Sydney, or that unscrupulous lenders
and careless borrowers in the United States can upend economic
expectations everywhere else in the world, no matter how carefully
others have planned, means that there are many more moving parts that
affect each other. And from this scale of practical effects, complexity
emerges. New things too were ever under the sun; but the systematic
application of knowledge to the creation of new knowledge, innovation to
innovation, and information to making more information has become
pervasive; and with it the knowledge that next year will be very
different than this. The Web, after all, is less than a generation old.

These two features−the global scale of interdependence of human action,
and the systematic acceleration of innovation, make contemporary life a
bit like a slow motion disaster, in one important respect. Its very
unpredictability makes it unwise to build systems that take too much
away from what human beings do best: look, think, innovate, adapt,
discuss, learn, and repeat. That is why we have seen many more systems
take on a loose, human centric model in the last decade and a half: from
the radical divergence of Toyota's production system from the highly
structured model put in place by Henry Ford, to the Internet's radical
departure from the AT&T system that preceded it, and on to the way
Wikipedia constructs human knowledge on the fly, incrementally, in ways
that would have been seen, until recently, as too chaotic ever to work
(and are still seen so be many). But it is time we acknowledge that
systems work best by making work human.

### Modern Times

Modern times were hard enough. Trains and planes, telegraph and
telephone, all brought many people into the same causal space. The
solution to this increased complexity in the late 19th, early 20th
century was to increase the role of structure and improve its design.
During the first two-thirds of the twentieth century, this type of
rationalization took the form of ever-more complex managed systems, with
crisp specification of roles, lines of authority, communication and
control.

In business, this rationalization was typified by Fredrick Taylor's
Scientific Management, later embodied in Henry Ford's assembly line. The
ambition of these approaches was to specify everything that needed doing
in minute detail, to enforce it through monitoring and rewards, and
later to build it into the very technology of work−the assembly line.
The idea was to eliminate human error and variability in the face of
change by removing thinking to the system, and thus neutralizing the
variability of the human beings who worked it. Few images captured that
time, and what it did to humanity, more vividly than Charlie Chaplin's
assembly line worker in Modern Times.

At the same time, government experienced the rise of bureaucratization
and the administrative state. Nowhere was this done more brutally than
in the totalitarian states of mid-century. But the impulse to build
fully-specified systems, designed by experts, monitored and controlled
so as to limit human greed and error and to manage uncertainty, was
basic and widespread. It underlay the development of the enormously
successful state bureaucracies that responded to the Great Depression
with the New Deal. It took shape in the Marshall Plan to pull Europe out
of the material abyss into which it had been plunged by World War II,
and shepherded Japan's industrial regeneration from it. In technical
systems too, we saw in mid-century marvels like the AT&T telephone
system and the IBM mainframe. For a moment in history, these large scale
managed systems were achieving efficiencies that seemed to overwhelm
competing models: from the Tennessee Valley Authority to Sputnik, from
Watson's IBM to General Motors. Yet, to list these paragons from today's
perspective is already to presage the demise of the belief in their
inevitable victory.

The increasing recognition of the limits of command-and-control systems
led to a new approach; but it turned out to be a retrenchment, not an
abandonment, of the goal of perfect rationalization of systems design,
which assumed much of the human away. What replaced planning and control
in these systems was the myth of perfect markets. This was achieved
through a hyper-simplification of human nature, wedded to mathematical
modeling of what hyper-simplified selfish rational actors, looking only
to their own interests, would do under diverse conditions. This approach
was widespread and influential; it still is. And yet it led to such
unforgettable gems as trying to understand why people do, or do not, use
condoms by writing sentences like: "The expected utility (EU) of unsafe
sex for m and for f is equal to the benefits (B) of unsafe sex minus its
expected costs, and is given by EUm = B - C(1-Pm)(Pf) and EUf = B -
C(1-Pf)(Pm)," and believing that you will learn anything useful about
lust and desire, recklessness and helplessness, or how to slow down the
transmission of AIDS. Only by concocting such a thin model of
humanity−no more than the economists' utility curve−and neglecting any
complexities of social interactions that could not be conveyed through
prices, could the appearance of rationalization be maintained. Like
bureaucratic rationalization, perfect-market rationalization also had
successes. But, like its predecessor, its limits as an approach to human
systems design are becoming cleare

### Work, Trust and Play

Pricing perfectly requires perfect information. And perfect information,
while always an illusion, has become an ever receding dream in a world
of constant, rapid change and complex global interactions. What we are
seeing instead is the rise of human systems that increasingly shy away
from either control or perfect pricing. Not that there isn't control.
Not that there aren't markets. And not that either of these approaches
to coordinating human action will disappear. But these managed systems
are becoming increasingly interlaced with looser structures, which
invite and enable more engaged human action by drawing on intrinsic
motivations and social relations. Dress codes and a culture of play in
the workplace in Silicon Valley, like the one day per week that Google
employees can use to play at whatever ideas they like, do not exist to
make the most innovative region in the United States a Ludic paradise,
gratifying employees at the expense of productivity, but rather to
engage the human and social in the pursuit of what is, in the long term,
the only core business competency−innovation. Wikipedia has eclipsed all
the commercial encyclopedias except Britannica not by issuing a large
IPO and hiring the smartest guys in the room, but by building an open
and inviting system that lets people learn together and pursue their
passion for knowledge, and each other's company.

The set of human systems necessary for action in this complex,
unpredictable set of conditions, combining rationalization with human
agency, learning and adaptation, is as different from managed systems
and perfect markets as the new Toyota is from the old General Motors, or
as the Internet now is from AT&T then. The hallmarks of these newer
systems are: (a) location of authority and practical capacity to act at
the edges of the system, where potentialities for sensing the
environment, identifying opportunities and challenges to action and
acting upon them, are located; (b) an emphasis on the human: on trust,
cooperation, judgment and insight; (c) communication over the lifetime
of the interaction; and (d) loosely-coupled systems: systems in which
the regularities and dependencies among objects and processes are less
strictly associated with each other; where actions and interactions can
occur through multiple systems simultaneously, have room to fail,
maneuver, and be reoriented to fit changing conditions and new learning,
or shift from one system to another to achieve a solution.

Consider first of all the triumph of Toyota over the programs of Taylor
and Ford. Taylorism was typified by the ambition to measure and specify
all human and material elements of the production system. The ambition
of scientific management was to offer a single, integrated system where
all human variance (the source of slothful shirking and inept error)
could be isolated and controlled. Fordism took that ambition and
embedded the managerial knowledge in the technological platform of the
assembly line, guided by a multitude of rigid task specifications and
routines. Toyota Production System, by comparison, has a substantially
smaller number of roles that are also more loosely defined, with a
reliance on small teams where each team member can perform all tasks,
and who are encouraged to experiment, improve, fail, adapt, but above
all communicate. The system is built on trust and a cooperative dynamic.
The enterprise functions through a managerial control system, but also
through social cooperation mechanisms built around teamwork and trust.
However, even Toyota might be bested in this respect by the even more
loosely coupled networks of innovation and supply represented by
Taiwanese original-design manufacturers.

But let us also consider the system in question that has made this work
possible, the Internet, and compare it to the design principles of the
AT&T network in its heyday. Unlike the Internet, AT&T's network was
fully managed. Mid-century, the company even retained ownership of the
phones at the endpoints, arguing that it needed to prohibit customers
from connecting unlicensed phones to the system (ostensibly to ensure
proper functioning of the networking and monitoring of customer
behavior, although it didn't hurt either that this policy effectively
excluded competitors). This generated profit, but any substantial
technical innovations required the approval of management and a
re-engineering of the entire network. The Internet, on the other hand,
was designed to be as general as possible. The network hardware merely
delivers packets of data using standardized addressing information. The
hard processing work−manipulating a humanly-meaningful communication (a
letter or a song, a video or a software package) and breaking it up into
a stream of packets−was to be done by its edge devices, in this case
computers owned by users. This system allowed the breathtaking rate of
innovation that we have seen, while also creating certain
vulnerabilities in online security.

These vulnerabilities have led some to argue that a new system to manage
the Internet is needed. We see first of all that doubts about trust and
security on the Internet arise precisely because the network was
originally designed for people who could more-or-less trust each other,
and offloaded security from the network to the edges. As the network
grew and users diversified, trust (the practical belief that other human
agents in the system were competent and benign, or at least sincere)
declined. This decline was met with arguments in favor of building
security into the technical system, both at its core, in the network
elements themselves, and at its periphery, through "trusted computing."
A "trusted computer" will, for example, not run a program or document
that its owner wants to run, unless it has received authorization from
some other locus: be it the copyright owner, the virus protection
company, or the employer. This is thought to be the most completely
effective means of preventing copyright infringement or system failure,
and preserving corporate security (these are the main reasons offered
for implementing such systems). Trusted computing in this form is the
ultimate reversal of the human-centric, loosely-coupled design approach
of the Internet. Instead of locating authority and capacity to act at
the endpoints, where human beings are located and can make decisions
about what is worthwhile, it implements the belief that
machines−technical systems−are trustworthy, while their human users are
malevolent, incompetent, or both.

### Reintroducing the Human

Taylorism, the Bell system and trusted computing are all efforts to
remove human agency from action and replace it with well-designed,
tightly-bound systems. That is, the specifications and regularities of
the system are such that they control or direct action and learning over
time. Human agency, learning, communication and adaptation are minimized
in managed systems, if not eliminated, and the knowledge in the system
comes from the outside, from the designer, in the initial design over
time, and through observation of the system's performance by someone
standing outside its constraints−a manager or systems designer. By
contrast, loosely-coupled systems affirmatively eschew this level of
control, and build in room for human agency, experimentation, failure,
communication, learning and adaptation. Loose-coupling is central to the
new systems. It is a feature of system design that leaves room for human
agency over time, only imperfectly constraining and enabling any given
action by the system itself. By creating such domains of human agency,
system designers are accepting the limitations of design and foresight,
and building in the possibilities of learning over time through action
in the system, by agents acting within

To deal with the new complexity of contemporary life we need to
re-introduce the human into the design of systems. We must put the soul
back into the system. If years of work on artificial intelligence have
taught us anything, it is that what makes for human insight is extremely
difficult to replicate or systematize. At the center of these new
systems, then, sits a human being who has a capacity to make judgments,
experiment, learn and adapt. But enabling human agency also provides
scope of action for human frailty. Although this idea is most alien to
the mainstream of system design in the twentieth century, we must now
turn our attention to building systems that support human sociality−our
ability to think of others and their needs, and to choose for ourselves
goals consistent with a broader social concern than merely our own
self-interest. The challenge of the near future is to build systems that
will allow us to be largely free to inquire, experiment, learn and
communicate, that will encourage us to cooperate, and that will avoid
the worst of what human beings are capable of, and elicit what is best.
Free software, Wikipedia, Creative Commons and the thousands of emerging
human practices of productive social cooperation in the networked
information economy give us real existence proofs that human-centric
systems can not merely exist, but thrive, as can the human beings and
social relations that make them.
